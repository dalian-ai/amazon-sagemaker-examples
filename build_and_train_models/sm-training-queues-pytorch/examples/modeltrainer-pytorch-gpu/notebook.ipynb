{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad542e7-9ef8-41d1-9d6c-3c6c2efb7f19",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fine-tune LLM with PyTorch FSDP and QLora on Amazon SageMaker AI using ModelTrainer\n",
    "\n",
    "In this notebook, we fine-tune LLM on Amazon SageMaker AI, using Python scripts and SageMaker ModelTrainer for executing a training job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eca016c-d4fa-4213-a7b3-03b449551449",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907944ea-dbfb-4de0-9e13-1fd28c901031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -r ./scripts/requirements.txt --upgrade\n",
    "! pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6c9e5c-c57c-42cd-baf4-e139422cc147",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5fc3d",
   "metadata": {},
   "source": [
    "## Setup Configuration file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce51663-0171-4d54-b16e-f85e3cadb692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"../..\"))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "os.environ[\"model_id\"] = model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b95b61-8666-4015-bf2e-fcf68ce38c5b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82089d28-b97a-4956-83fb-d8c46d44fdb5",
   "metadata": {},
   "source": [
    "## Visualize and upload the dataset\n",
    "\n",
    "We are going to load [FreedomIntelligence/medical-o1-reasoning-SFT](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc5fa8-51b5-419c-9a87-784022e23e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T09:24:55.572481Z",
     "start_time": "2023-11-15T09:24:52.575954Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d973b5e-ac00-4b10-8425-5c4ca4b31f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d481791d-9c86-4d32-a39a-918aff5e432f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=\"train[:1000]\")\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f69e7cf-1b66-4246-90d4-b855a2f8bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Number of train elements: \", len(train))\n",
    "print(\"Number of test elements: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368c020-e9a3-48b3-a53b-45404bba9482",
   "metadata": {},
   "source": [
    "Create a prompt template and load the dataset with a random sample to try summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb8edd-35c0-4cf1-82d3-54417bdabd6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:01.435195Z",
     "start_time": "2023-09-03T00:02:01.429794Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def prepare_dataset(sample):\n",
    "    system_text = (\n",
    "        \"You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\\n\"\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context.\\n\"\n",
    "        \"Write a response that appropriately completes the request.\\n\"\n",
    "        \"Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\"\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": system_text})\n",
    "    messages.append({\"role\": \"user\", \"content\": sample[\"Question\"]})\n",
    "\n",
    "    # Use different tags that won't be detected by the template\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"\\n[REASONING_START]\\n{sample['Complex_CoT']}\\n[REASONING_END]\\n{sample['Response']}\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    # Replace with actual think tags after template processing\n",
    "    formatted_text = formatted_text.replace(\"[REASONING_START]\", \"<think>\")\n",
    "    formatted_text = formatted_text.replace(\"[REASONING_END]\", \"</think>\")\n",
    "\n",
    "    sample[\"text\"] = formatted_text\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f0a32-96c2-4285-b593-3de211f79933",
   "metadata": {},
   "source": [
    "Use the Hugging Face Trainer class to fine-tune the model. Define the hyperparameters we want to use. We also create a DataCollator that will take care of padding our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cbedd-7403-467e-8cc6-1d2550d8b8e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:10.364459Z",
     "start_time": "2023-09-03T00:02:09.672705Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from random import randint\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset})\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset, remove_columns=list(train_dataset.features)\n",
    ")\n",
    "\n",
    "print(train_dataset[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "val_dataset = dataset[\"val\"].map(\n",
    "    prepare_dataset, remove_columns=list(val_dataset.features)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e667af-8197-4d2f-8432-82db6a1d3006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T16:46:36.592759Z",
     "iopub.status.busy": "2024-12-17T16:46:36.591798Z",
     "iopub.status.idle": "2024-12-17T16:46:36.603128Z",
     "shell.execute_reply": "2024-12-17T16:46:36.598965Z",
     "shell.execute_reply.started": "2024-12-17T16:46:36.592728Z"
    }
   },
   "source": [
    "### Upload to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f29e5-4aed-4939-8d51-ad3c5268299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db05863-3acb-483b-8e34-2aacbdbc68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0321-1bd5-4c62-845a-bb1b9a3891a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "if default_prefix:\n",
    "    input_path = f\"{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft-batch\"\n",
    "else:\n",
    "    input_path = f\"datasets/llm-fine-tuning-modeltrainer-sft-batch\"\n",
    "\n",
    "# Save datasets to s3\n",
    "# We will fine tune only with 20 records due to limited compute resource for the workshop\n",
    "train_dataset.to_json(\"./data/train/dataset.json\", orient=\"records\")\n",
    "val_dataset.to_json(\"./data/val/dataset.json\", orient=\"records\")\n",
    "\n",
    "s3_client.upload_file(\"./data/train/dataset.json\", bucket_name, f\"{input_path}/train/dataset.json\")\n",
    "train_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/train/dataset.json\"\n",
    "s3_client.upload_file(\"./data/val/dataset.json\", bucket_name, f\"{input_path}/val/dataset.json\")\n",
    "val_dataset_s3_path = f\"s3://{bucket_name}/{input_path}/val/dataset.json\"\n",
    "\n",
    "shutil.rmtree(\"./data\")\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(val_dataset_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9c237-28bd-474e-9444-94aaea8e6979",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457beda-117d-4782-9f04-0680c199e98a",
   "metadata": {},
   "source": [
    "## Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5a09e-97de-4935-82c5-b56445e057fd",
   "metadata": {},
   "source": [
    "We are now ready to fine-tune our model. We will use the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) from transfomers to fine-tune our model. We prepared a script [train.py](./scripts/train.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training.\n",
    "\n",
    "For configuration we use `TrlParser`, that allows us to provide hyperparameters in a `yaml` file. This yaml will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning the model on `ml.g5.12xlarge`. We are saving the config file as `args.yaml` and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5aaaf-7d2f-4aae-87af-1b9e6b11b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./args.yaml <<EOF\n",
    "model_id: \"${model_id}\"       # Hugging Face model id\n",
    "# sagemaker specific parameters\n",
    "output_dir: \"/opt/ml/model\"                       # path to where SageMaker will upload the model \n",
    "train_dataset_path: \"/opt/ml/input/data/train/\"   # path to where FSx saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/val/\"     # path to where FSx saves test dataset\n",
    "# training parameters\n",
    "lora_r: 8\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.1                 \n",
    "learning_rate: 2e-4                    # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 2         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "gradient_checkpointing: true           # use gradient checkpointing\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: false                            # use tf32 precision\n",
    "fsdp: \"full_shard auto_wrap offload\"\n",
    "fsdp_config: \n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    cpu_ram_efficient_loading: true\n",
    "    offload_params: true\n",
    "    forward_prefetch: false\n",
    "    use_orig_params: true\n",
    "merge_weights: true                    # merge weights in the base model\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd27c7-d367-43b1-8b61-ce15e0e262c1",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70937e95-114e-40e1-b26a-49cc1cbd803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "if default_prefix:\n",
    "    input_path = f\"s3://{bucket_name}/{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft-batch\"\n",
    "else:\n",
    "    input_path = f\"s3://{bucket_name}/datasets/llm-fine-tuning-modeltrainer-sft-batch\"\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"args.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "os.remove(\"./args.yaml\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329683c-6662-45d3-b864-9cb575f92599",
   "metadata": {},
   "source": [
    "## Create the ModelTrainer\n",
    "\n",
    "Below the Estimator will will be used to submit the jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178118a-0f45-4e5f-9bb1-7e5dee146b62",
   "metadata": {},
   "source": [
    "#### Get PyTorch image_uri\n",
    "\n",
    "We are going to use the native PyTorch container image, pre-built for Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5a03c-7660-4729-bf98-67ecb8ffa508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.config import load_sagemaker_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaaf81c-e8fb-4e42-a90d-50c2c55047bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "configs = load_sagemaker_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cecfd-e640-4527-99d4-cb3cec9093b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.g6.12xlarge\"\n",
    "instance_count = 1\n",
    "\n",
    "instance_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df7700-7c66-4af8-aea0-da0e5af493bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=sagemaker_session.boto_session.region_name,\n",
    "    version=\"2.6\",\n",
    "    instance_type=instance_type,\n",
    "    image_scope=\"training\"\n",
    ")\n",
    "\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a5e6f-73b8-4f66-8cd1-d2425bbaa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cabb4d-b0b2-498c-95cb-41ed7d05ee65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-03T00:02:21.382486Z",
     "start_time": "2023-09-03T00:02:20.962208Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import Compute, OutputDataConfig, SourceCode, StoppingCondition\n",
    "from sagemaker.modules.distributed import Torchrun\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"./scripts\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    entry_script=\"train.py\",\n",
    ")\n",
    "\n",
    "# Define the compute\n",
    "compute_configs = Compute(\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    keep_alive_period_in_seconds=0\n",
    ")\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f\"train-{model_id.split('/')[-1].replace('.', '-')}-sft-batch\"\n",
    "\n",
    "# define OutputDataConfig path\n",
    "if default_prefix:\n",
    "    output_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\n",
    "else:\n",
    "    output_path = f\"s3://{bucket_name}/{job_name}\"\n",
    "\n",
    "# Define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=image_uri,\n",
    "    source_code=source_code,\n",
    "    base_job_name=job_name,\n",
    "    compute=compute_configs,\n",
    "    distributed=Torchrun(),\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=7200),\n",
    "    hyperparameters={\n",
    "        \"config\": \"/opt/ml/input/data/config/args.yaml\"  # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    output_data_config=OutputDataConfig(s3_output_path=output_path),\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a386bd9-172c-485c-af45-ebc1d126470b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.modules.configs import InputData\n",
    "\n",
    "# Pass the input data\n",
    "train_input = InputData(\n",
    "    channel_name=\"train\",\n",
    "    data_source=train_dataset_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "val_input = InputData(\n",
    "    channel_name=\"val\",\n",
    "    data_source=val_dataset_s3_path,  # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "config_input = InputData(\n",
    "    channel_name=\"config\",\n",
    "    data_source=train_config_s3_path, # S3 path where training data is stored\n",
    ")\n",
    "\n",
    "# Check input channels configured\n",
    "TRAINING_INPUTS = [train_input, val_input, config_input]\n",
    "TRAINING_INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c03ae1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3defa6",
   "metadata": {},
   "source": [
    "## Queue Some Training Jobs\n",
    "This section and the following are intended to be used interactively so that you can explore how to use the SageMaker Python SDK to submit jobs to your Batch queues. Let's start by selecting which queue to submit to.\n",
    "\n",
    "### Select the Queue to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.aws_batch.training_queue import TrainingQueue\n",
    "\n",
    "# Set the queue type to use for your job submission\n",
    "SMTJ_BATCH_QUEUE = \"ml-g6-12xlarge-queue\"\n",
    "\n",
    "# Construct the queue object using the SageMaker Python SDK\n",
    "queue = TrainingQueue(SMTJ_BATCH_QUEUE)\n",
    "print(f\"Using queue: {queue.queue_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae1c34",
   "metadata": {},
   "source": [
    "### Submit your jobs\n",
    "In the next cell, we are going to submit 2 Training jobs in the queue\n",
    "1. LOW PRIORITY\n",
    "3. MEDIUM PRIORITY\n",
    "\n",
    "We are going to use the API `submit` to submit all the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_1 = job_name + \"-low-pri\"\n",
    "queued_job_1 = queue.submit(\n",
    "    model_trainer, TRAINING_INPUTS, job_name_1, priority=5, share_identifier=\"LOWPRI\"\n",
    ")\n",
    "\n",
    "job_name_2 = job_name + \"-mid-pri\"\n",
    "queued_job_2 = queue.submit(\n",
    "    model_trainer, TRAINING_INPUTS, job_name_2, priority=3, share_identifier=\"MIDPRI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db05cd",
   "metadata": {},
   "source": [
    "## Display the Status of Running and 'In Queue' Jobs\n",
    "We can use the job queue list and job queue snapshot APIs to programmaticaly view a snapshot of the jobs that the queue will run next. Keep in mind that for fair-share queues this ordering is dynamic and occassionally needs to be refreshed as new jobs are submitted to the queue or as share usage changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtj_batch_utils.queue_utils import print_queue_state\n",
    "\n",
    "print_queue_state(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e64d2e",
   "metadata": {},
   "source": [
    "### Submit an additional job\n",
    "In the next cell, we are going to submit an additional job to the queue, by using the API `submit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_3 = job_name + \"-high-pri\"\n",
    "queued_job_3 = queue.submit(\n",
    "    model_trainer, TRAINING_INPUTS, job_name_3, priority=1, share_identifier=\"HIGHPRI\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc635a",
   "metadata": {},
   "source": [
    "## Display the Status of Running and 'In Queue' Jobs\n",
    "Now we are going to see another runnable job. Given that the last job has high priority, it will be run before the `MIDPRI` and `LOWPRI` jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60417f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtj_batch_utils.queue_utils import print_queue_state\n",
    "\n",
    "print_queue_state(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864e6e1",
   "metadata": {},
   "source": [
    "## Cancel a Job in the Queue\n",
    "This next cell shows how to cancel an in queue job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b4b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable_jobs = queue.list_jobs(status=\"RUNNABLE\")\n",
    "if runnable_jobs:\n",
    "    for job in runnable_jobs:\n",
    "        job_to_cancel = job\n",
    "        print(f\"Cancelling job: {job_to_cancel.describe().get('jobName', '')}\")\n",
    "        job_to_cancel.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgtj-queue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
